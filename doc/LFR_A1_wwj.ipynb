{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e99894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import scipy.optimize as optim\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "from IPython.display import Markdown, display\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "from tabulate import tabulate\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa518bf",
   "metadata": {},
   "source": [
    "# Learning fair representations (LFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5b982",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7133619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data\n",
    "data = pd.read_csv('data/processed-compas-scores-two-years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20689d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "y = np.array(data[:,-1]).flatten()\n",
    "data = data[:,:-1]\n",
    "sensitive = data[:,0]\n",
    "data = preprocessing.scale(data)\n",
    "data = data[:,1:]\n",
    "\n",
    "# Split data into sensitive and nonsensitive data (sensitive --> race: Caucasian)\n",
    "\n",
    "sensitive_idx = np.array(np.where(sensitive==1))[0].flatten()\n",
    "nonsensitive_idx = np.array(np.where(sensitive!=1))[0].flatten()\n",
    "data_sensitive = data[sensitive_idx,:]\n",
    "data_nonsensitive = data[nonsensitive_idx,:]\n",
    "y_sensitive = y[sensitive_idx]\n",
    "y_nonsensitive = y[nonsensitive_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c00b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sensitive data into training, validation, and testing sets\n",
    "\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(data_sensitive, y_sensitive, test_size= 0.2, random_state=42)\n",
    "X_train_s, X_valid_s, y_train_s, y_valid_s = train_test_split(X_train_s, y_train_s, test_size = 0.25, random_state=42)\n",
    "# split non-sensitive data into training, validation, and testing sets\n",
    "\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(data_nonsensitive, y_nonsensitive, test_size= 0.2, random_state=42)\n",
    "X_train_n, X_valid_n, y_train_n, y_valid_n = train_test_split(X_train_n, y_train_n, test_size = 0.25, random_state=42)\n",
    "# create final training, validation, and testing sets\n",
    "\n",
    "X_train = np.concatenate((X_train_s, X_train_n))\n",
    "X_valid = np.concatenate((X_valid_s, X_valid_n))\n",
    "X_test = np.concatenate((X_test_s, X_test_n))\n",
    "\n",
    "Y_train = np.concatenate((y_train_s, y_train_n))\n",
    "Y_valid = np.concatenate((y_valid_s, y_valid_n))\n",
    "Y_test = np.concatenate((y_test_s, y_test_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19914dd",
   "metadata": {},
   "source": [
    "## LFR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca1589ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the distance matrix\n",
    "def distances(X, v, alpha, N, P, k):\n",
    "    dists = np.zeros((N, k))\n",
    "    for i in range(N):\n",
    "        for p in range(P):\n",
    "            for j in range(k):    \n",
    "                dists[i, j] += (X[i, p] - v[j, p]) * (X[i, p] - v[j, p]) * alpha[p]\n",
    "    return dists\n",
    "\n",
    "# this function returns the M_nk\n",
    "def M_nk(dists, N, k):\n",
    "    M_nk = np.zeros((N, k))\n",
    "    exp = np.zeros((N, k))\n",
    "    denom = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            exp[i, j] = np.exp(-1 * dists[i, j])\n",
    "            denom[i] += exp[i, j]\n",
    "        for j in range(k):\n",
    "            if denom[i]:\n",
    "                M_nk[i, j] = exp[i, j] / denom[i]\n",
    "            else:\n",
    "                M_nk[i, j] = exp[i, j] / 1e-6\n",
    "    return M_nk\n",
    " \n",
    "# this function returns the M_k array\n",
    "def M_k(M_nk, N, k):\n",
    "    M_k = np.zeros(k)\n",
    "    for j in range(k):\n",
    "        for i in range(N):\n",
    "            M_k[j] += M_nk[i, j]\n",
    "        M_k[j] /= N\n",
    "    return M_k\n",
    "\n",
    "# this function reconstructs of X to x_n_hat and L_x\n",
    "def x_n_hat(X, M_nk, v, N, P, k):\n",
    "    x_n_hat = np.zeros((N, P))\n",
    "    L_x = 0.0\n",
    "    for i in range(N):\n",
    "        for p in range(P):\n",
    "            for j in range(k):\n",
    "                x_n_hat[i, p] += M_nk[i, j] * v[j, p]\n",
    "            L_x += (X[i, p] - x_n_hat[i, p]) * (X[i, p] - x_n_hat[i, p])\n",
    "    return x_n_hat, L_x\n",
    "\n",
    "# this function returns a list of prediction\n",
    "def yhat(M_nk, y, w, N, k):\n",
    "    yhat = np.zeros(N)\n",
    "    L_y = 0.0\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            yhat[i] += M_nk[i, j] * w[j]\n",
    "        yhat[i] = 1e-6 if yhat[i] <= 0 else yhat[i]\n",
    "        yhat[i] = 0.999 if yhat[i] >= 1 else yhat[i]\n",
    "        L_y += -1 * y[i] * np.log(yhat[i]) - (1.0 - y[i]) * np.log(1.0 - yhat[i])\n",
    "    return yhat, L_y\n",
    "\n",
    "\n",
    "# this function returns the objective function we want to minimize\n",
    "def LFR_objective(params, data_sensitive, data_nonsensitive, y_sensitive, \n",
    "        y_nonsensitive,  k=10, A_x = 1e-4, A_y = 0.1, A_z = 1000):\n",
    "    LFR_objective.iters += 1 \n",
    "    Ns, P = data_sensitive.shape\n",
    "    Nns, _ = data_nonsensitive.shape\n",
    "    \n",
    "    alpha0 = params[:P]\n",
    "    alpha1 = params[P : 2 * P]\n",
    "    w = params[2 * P : (2 * P) + k]\n",
    "    v = np.matrix(params[(2 * P) + k:]).reshape((k, P))\n",
    "        \n",
    "    dists_sensitive = distances(data_sensitive, v, alpha0, Ns, P, k)\n",
    "    dists_nonsensitive = distances(data_nonsensitive, v, alpha1, Nns, P, k)\n",
    "\n",
    "    M_nk_sensitive = M_nk(dists_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive = M_nk(dists_nonsensitive, Nns, k)\n",
    "    \n",
    "    M_k_sensitive = M_k(M_nk_sensitive, Ns, k)\n",
    "    M_k_nonsensitive = M_k(M_nk_nonsensitive, Nns, k)\n",
    "    \n",
    "    L_z = 0.0\n",
    "    for j in range(k):\n",
    "        L_z += abs(M_k_sensitive[j] - M_k_nonsensitive[j])\n",
    "\n",
    "    x_n_hat_sensitive, L_x_sen = x_n_hat(data_sensitive, M_nk_sensitive, v, Ns, P, k)\n",
    "    x_n_hat_nonsensitive, L_x_nsen = x_n_hat(data_nonsensitive, M_nk_nonsensitive, v, Nns, P, k)\n",
    "    L_x = L_x_sen + L_x_nsen\n",
    "\n",
    "    yhat_sensitive, L_y_sen = yhat(M_nk_sensitive, y_sensitive, w, Ns, k)\n",
    "    yhat_nonsensitive, L_y_nsen = yhat(M_nk_nonsensitive, y_nonsensitive, w, Nns, k)\n",
    "    L_y = L_y_sen + L_y_nsen\n",
    "\n",
    "    objective = A_x * L_x + A_y * L_y + A_z * L_z\n",
    "\n",
    "    return objective\n",
    "\n",
    "LFR_objective.iters = 0\n",
    "\n",
    "def LFR(X_train_s, X_train_n, y_train_s, y_train_n, K=10, A_x = 1e-4, A_y = 0.1, A_z = 1000, iter = 100):\n",
    "    rez = np.random.uniform(size=X_train_s.shape[1] * 2 + K + X_train_s.shape[1] * K)\n",
    "    bnd = []\n",
    "    for i, k2 in enumerate(rez):\n",
    "        if i < X_train_s.shape[1] * 2 or i >= X_train_s.shape[1] * 2 + K:\n",
    "            bnd.append((None, None))\n",
    "        else:\n",
    "            bnd.append((0, 1))\n",
    "    \n",
    "    # minimize the metric by parameters alpha, w and v\n",
    "    para, min_L, d = optim.fmin_l_bfgs_b(LFR_objective, x0=rez, epsilon=1e-5, \n",
    "                                         args=(X_train_s, X_train_n, y_train_s, y_train_n, K, A_z, A_x, A_y), \n",
    "                                         bounds = bnd, approx_grad=True, \n",
    "                                         maxfun=iter, maxiter=iter)\n",
    "    \n",
    "    return para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b813d",
   "metadata": {},
   "source": [
    "## Evaluation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95ee1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function defines the threshold for y_n_hat to be 0 or 1\n",
    "def predic_category(y):\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 0.5:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return y\n",
    "\n",
    "# this function calculate y_n_hat by using the best parameters\n",
    "def predict(params, data_sensitive, data_nonsensitive, k=10):\n",
    "    \n",
    "    Ns, P = data_sensitive.shape\n",
    "    Nns, _ = data_nonsensitive.shape\n",
    "    \n",
    "    # form parameters in new forms\n",
    "    alpha0 = params[:P]\n",
    "    alpha1 = params[P : 2 * P]\n",
    "    w = params[2 * P : (2 * P) + k]\n",
    "    v = np.matrix(params[(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    dists_sensitive = distances(data_sensitive, v, alpha0, Ns, P, k)\n",
    "    dists_nonsensitive = distances(data_nonsensitive, v, alpha1, Nns, P, k)\n",
    "\n",
    "    M_nk_sensitive = M_nk(dists_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive = M_nk(dists_nonsensitive, Nns, k)\n",
    "    \n",
    "    M_k_sensitive = M_k(M_nk_sensitive, Ns, k)\n",
    "    M_k_nonsensitive = M_k(M_nk_nonsensitive, Nns, k)\n",
    "    \n",
    "    # make predictions for sensitive data\n",
    "    yhat_sensitive = np.zeros(Ns)\n",
    "    for i in range(Ns):\n",
    "        for j in range(k):\n",
    "            yhat_sensitive[i] += M_nk_sensitive[i, j] * w[j]\n",
    "        yhat_sensitive[i] = 1e-6 if yhat_sensitive[i] <= 0 else yhat_sensitive[i]\n",
    "        yhat_sensitive[i] = 0.999 if yhat_sensitive[i] >= 1 else yhat_sensitive[i]\n",
    "        \n",
    "    # make predictions for nonsensitive data\n",
    "    yhat_nonsensitive = np.zeros(Nns)\n",
    "    for i in range(Nns):\n",
    "        for j in range(k):\n",
    "            yhat_nonsensitive[i] += M_nk_nonsensitive[i, j] * w[j]\n",
    "        yhat_nonsensitive[i] = 1e-6 if yhat_nonsensitive[i] <= 0 else yhat_nonsensitive[i]\n",
    "        yhat_nonsensitive[i] = 0.999 if yhat_nonsensitive[i] >= 1 else yhat_nonsensitive[i]\n",
    "        \n",
    "    final_y_s = predic_category(yhat_sensitive)\n",
    "    final_y_n = predic_category(yhat_nonsensitive)\n",
    "    \n",
    "    return final_y_s, final_y_n\n",
    "\n",
    "def calc_accuracy(y_sen, y_nsen, y_sen_label, y_nsen_label):\n",
    "    y_sen_df = pd.DataFrame(y_sen)\n",
    "    y_nsen_df = pd.DataFrame(y_nsen)\n",
    "    y_label = pd.DataFrame(y_sen_label).append(pd.DataFrame(y_nsen_label))\n",
    "    y_df = y_sen_df.append(y_nsen_df)\n",
    "    \n",
    "    acc_sen = accuracy_score(y_sen_label, y_sen_df)\n",
    "    acc_nsen = accuracy_score(y_nsen_label, y_nsen_df)\n",
    "    total_accuracy = accuracy_score(y_label, y_df)\n",
    "    \n",
    "    return acc_sen, acc_nsen, total_accuracy\n",
    "\n",
    "def calc_calibration(acc_sen, acc_nsen):\n",
    "    return abs(acc_sen - acc_nsen)\n",
    "\n",
    "def get_model_performance(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, matrix, f1\n",
    "\n",
    "def plot_model_performance(y_pred_s, y_pred_n, y_pred, y_true_s, y_true_n, y_true):\n",
    "    accuracy_s, matrix_s, f1_s = get_model_performance(y_true_s, y_pred_s)\n",
    "\n",
    "    display(Markdown('#### Sensitive data (Caucasians):'))\n",
    "    print(f'Accuracy: {accuracy_s}')\n",
    "    print(f'F1 score: {f1_s}')\n",
    "    \n",
    "    accuracy_n, matrix_n, f1_n = get_model_performance(y_true_n, y_pred_n)\n",
    "\n",
    "    display(Markdown('#### Nonsensitive data (African-Americans):'))\n",
    "    print(f'Accuracy: {accuracy_n}')\n",
    "    print(f'F1 score: {f1_n}')\n",
    "    \n",
    "    accuracy, matrix, f1 = get_model_performance(y_true, y_pred)\n",
    "\n",
    "    display(Markdown('#### All data:'))\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'F1 score: {f1}')\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    sns.heatmap(matrix_s, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title('Confusion Matrix (sensitive data)')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    sns.heatmap(matrix_n, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title('Confusion Matrix (nonsensitive data)')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title('Confusion Matrix (all data)')\n",
    "    \n",
    "def equal_opportunity_difference(y_test_s, y_test_n, pred_test_s, pred_test_n):\n",
    "    tpr_s = 0\n",
    "    for i in range(len(y_test_s)):\n",
    "        if y_test_s[i] == 1 and pred_test_s[i] == 1:\n",
    "            tpr_s += 1 \n",
    "    tpr_s = tpr_s/len(y_test_s)\n",
    "    tpr_n = 0\n",
    "    for i in range(len(y_test_n)):\n",
    "        if y_test_n[i] == 1 and pred_test_n[i] == 1:\n",
    "            tpr_n += 1 \n",
    "    tpr_n = tpr_n/len(y_test_n)\n",
    "    \n",
    "    equal_opportunity_difference = tpr_s - tpr_n\n",
    "    \n",
    "    return equal_opportunity_difference\n",
    "\n",
    "def avg_abs_odds_difference(y_test_s, y_test_n, pred_test_s, pred_test_n):\n",
    "    tpr_s = 0\n",
    "    for i in range(len(y_test_s)):\n",
    "        if y_test_s[i] == 1 and pred_test_s[i] == 1:\n",
    "            tpr_s += 1 \n",
    "    tpr_s = tpr_s/len(y_test_s)\n",
    "    tpr_n = 0\n",
    "    for i in range(len(y_test_n)):\n",
    "        if y_test_n[i] == 1 and pred_test_n[i] == 1:\n",
    "            tpr_n += 1 \n",
    "    tpr_n = tpr_n/len(y_test_n)\n",
    "    \n",
    "    fpr_s = 0\n",
    "    for i in range(len(y_test_s)):\n",
    "        if y_test_s[i] == 0 and pred_test_s[i] == 1:\n",
    "            fpr_s += 1 \n",
    "    fpr_s = fpr_s/len(y_test_s)\n",
    "    fpr_n = 0\n",
    "    for i in range(len(y_test_n)):\n",
    "        if y_test_n[i] == 0 and pred_test_n[i] == 1:\n",
    "            fpr_n += 1 \n",
    "    fpr_n = fpr_n/len(y_test_n)\n",
    "    \n",
    "    avg_abs_odds_diff = 0.5*(abs(fpr_s - fpr_n) + abs(tpr_s - tpr_n))\n",
    "    \n",
    "    return avg_abs_odds_diff\n",
    "\n",
    "def fair_metrics(pred_test_s, pred_test_n, pred_test, y_test_s, y_test_n, y_test):\n",
    "    \n",
    "    cols = ['calibration', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact']\n",
    "    obj_fairness = [[0,0,0,1]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    acc_sen, acc_nsen, total_accuracy = calc_accuracy(pred_test_s, pred_test_n, y_test_s, y_test_n)\n",
    "    \n",
    "    calibration = acc_sen - acc_nsen\n",
    "    \n",
    "    equal_opp_diff = equal_opportunity_difference(y_test_s, y_test_n, pred_test_s, pred_test_n)\n",
    "    \n",
    "    avg_abs_odds_diff = avg_abs_odds_difference(y_test_s, y_test_n, pred_test_s, pred_test_n)\n",
    "    \n",
    "    disparate_impact = acc_sen/acc_nsen\n",
    "    \n",
    "    row = pd.DataFrame([[calibration, equal_opp_diff, avg_abs_odds_diff, disparate_impact]],\n",
    "                           columns  = cols,\n",
    "                           index = ['Race']\n",
    "                          )\n",
    "    \n",
    "    fair_metrics = fair_metrics.append(row)\n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "    \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8]\n",
    "    bottom = [-1,-1,-1,0]\n",
    "    top = [1,1,1,2]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these four metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,4)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 4 metrics\"%(4 - sum(check))))\n",
    "\n",
    "    for i in range(0,4):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/4, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')\n",
    "\n",
    "def compare_models(pred_1_test_s, pred_1_test_n, pred_2_test_s, pred_2_test_n, y_test_s, y_test_n, y_PR_test_s, y_PR_test_n,\n",
    "                  fair_metrics_1, fair_metrics_2, model1, model2):\n",
    "    acc_1_sen, acc_1_nsen, total_accuracy_1 = calc_accuracy(pred_1_test_s, pred_1_test_n, y_test_s, y_test_n)\n",
    "    acc_2_sen, acc_2_nsen, total_accuracy_2 = calc_accuracy(pred_2_test_s, pred_2_test_n, y_PR_test_s, y_PR_test_n)\n",
    "\n",
    "    calibration_1 = fair_metrics_1.iloc[1]['calibration']\n",
    "    equal_opp_diff_1 = fair_metrics_1.iloc[1]['equal_opportunity_difference']\n",
    "    avg_abs_odds_diff_1 = fair_metrics_1.iloc[1]['average_abs_odds_difference']\n",
    "    disparate_impact_1 = fair_metrics_1.iloc[1]['disparate_impact']\n",
    "\n",
    "    calibration_2 = fair_metrics_2.iloc[1]['calibration']\n",
    "    equal_opp_diff_2 = fair_metrics_2.iloc[1]['equal_opportunity_difference']\n",
    "    avg_abs_odds_diff_2 = fair_metrics_2.iloc[1]['average_abs_odds_difference']\n",
    "    disparate_impact_2 = fair_metrics_2.iloc[1]['disparate_impact']\n",
    "    \n",
    "    print(tabulate([['accuracy', total_accuracy_1, total_accuracy_2], \n",
    "                ['calibration', calibration_1, calibration_2],\n",
    "                ['equal_opportunity_difference', equal_opp_diff_1, equal_opp_diff_2],\n",
    "                ['average_abs_odds_difference', avg_abs_odds_diff_1, avg_abs_odds_diff_2],\n",
    "                ['disparate_impact', disparate_impact_1, disparate_impact_2]], headers=['metric', model1, model2]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d72a5b",
   "metadata": {},
   "source": [
    "## Implementing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c9c1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for 100 iterations in 991.5338418483734 secs\n",
      "Finished for 200 iterations in 621.6066381931305 secs\n",
      "Finished for 300 iterations in 895.771886587143 secs\n",
      "Finished for 400 iterations in 1628.4575157165527 secs\n",
      "Finished for 500 iterations in 2639.538587808609 secs\n"
     ]
    }
   ],
   "source": [
    "iter_max = 500\n",
    "\n",
    "model_train_time = []\n",
    "train_Accuracy = []\n",
    "val_Accuracy = []\n",
    "train_Calibration = []\n",
    "val_Calibration = []\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "for i in range(100, iter_max+100, 100):\n",
    "\n",
    "    #model training\n",
    "    start = time.time()\n",
    "    #random.seed(1024); np.random.seed(1024)\n",
    "    final_parameters = LFR(X_train_s, X_train_n, y_train_s, y_train_n, 10, 1e-4, 0.1, 1000, iter = i)\n",
    "    model_train_time.append(time.time() - start)\n",
    "\n",
    "    #Train set accuracy and calibration\n",
    "    pred_train_s, pred_train_n = predict(final_parameters, X_train_s, X_train_n, 10)\n",
    "    acc_sen, acc_nsen, total_accuracy = calc_accuracy(pred_train_s, pred_train_n, y_train_s, y_train_n)\n",
    "    train_Accuracy.append(total_accuracy)\n",
    "\n",
    "    calibration = calc_calibration(acc_sen, acc_nsen)\n",
    "    train_Calibration.append(calibration)\n",
    "\n",
    "    #Validation set accuracy and calibration\n",
    "    pred_val_s, pred_val_n = predict(final_parameters, X_valid_s, X_valid_n, 10)\n",
    "    acc_sen, acc_nsen, total_accuracy = calc_accuracy(pred_val_s, pred_val_n, y_valid_s, y_valid_n)\n",
    "    val_Accuracy.append(total_accuracy)\n",
    "\n",
    "    calibration = calc_calibration(acc_sen, acc_nsen)\n",
    "    val_Calibration.append(calibration)\n",
    "\n",
    "    if total_accuracy > best_accuracy:\n",
    "        best_accuracy = total_accuracy\n",
    "        best_model = copy.deepcopy(final_parameters)\n",
    "\n",
    "    print(\"Finished for \" + str(i) + \" iterations in \" + str(time.time() - start) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d2f8a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 300, 400, 500]\n",
      "[989.1727020740509, 619.2910935878754, 894.5519225597382, 1625.0544106960297, 2635.091701745987]\n",
      "[0.5262160454832596, 0.543903979785218, 0.5764371446620341, 0.5906506632975363, 0.3063802905874921]\n",
      "[0.4895833333333333, 0.5539772727272727, 0.5785984848484849, 0.6070075757575758, 0.2793560606060606]\n",
      "[0.04422853170316443, 0.12143260046498949, 0.03705137571522832, 0.0687834718519027, 0.13752281757801685]\n",
      "[0.024153216002394007, 0.137366974021359, 0.08457179194643427, 0.12027979875437189, 0.09720762339386915]\n"
     ]
    }
   ],
   "source": [
    "filename = 'best_model.sav'\n",
    "pickle.dump(best_model, open(filename, 'wb'))\n",
    "\n",
    "iterations = [i for i in range(100, iter_max+100, 100)]\n",
    "\n",
    "print(iterations)\n",
    "print(model_train_time)\n",
    "print(train_Accuracy)\n",
    "print(val_Accuracy)\n",
    "print(train_Calibration)\n",
    "print(val_Calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642a27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'best_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e120a",
   "metadata": {},
   "source": [
    "Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23d3cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Caucasians is:  0.6320380650277557\n",
      "The accuracy for African-Americans is:  0.563254593175853\n",
      "The total accuracy is:  0.5906506632975363\n",
      "The calibration is:  0.0687834718519027\n"
     ]
    }
   ],
   "source": [
    "# get predictions for the training dataset\n",
    "\n",
    "pred_train_s, pred_train_n = predict(loaded_model, X_train_s, X_train_n, 10)\n",
    "\n",
    "# get accuracy for the training dataset\n",
    "\n",
    "acc_sen, acc_nsen, total_accuracy = calc_accuracy(pred_train_s, pred_train_n, y_train_s, y_train_n)\n",
    "\n",
    "print(\"The accuracy for Caucasians is: \", acc_sen)\n",
    "print(\"The accuracy for African-Americans is: \", acc_nsen)\n",
    "print(\"The total accuracy is: \", total_accuracy)\n",
    "\n",
    "# get calibration for the training dataset\n",
    "\n",
    "calibration = calc_calibration(acc_sen, acc_nsen)\n",
    "\n",
    "print(\"The calibration is: \", calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273aa10",
   "metadata": {},
   "source": [
    "Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04326adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Caucasians is:  0.6793349168646081\n",
      "The accuracy for African-Americans is:  0.5590551181102362\n",
      "The total accuracy is:  0.6070075757575758\n",
      "The calibration is:  0.12027979875437189\n"
     ]
    }
   ],
   "source": [
    "# get predictions for the validation dataset\n",
    "\n",
    "pred_val_s, pred_val_n = predict(loaded_model, X_valid_s, X_valid_n, 10)\n",
    "\n",
    "# get accuracy for the validation dataset\n",
    "\n",
    "acc_sen, acc_nsen, total_accuracy = calc_accuracy(pred_val_s, pred_val_n, y_valid_s, y_valid_n)\n",
    "\n",
    "print(\"The accuracy for Caucasians is: \", acc_sen)\n",
    "print(\"The accuracy for African-Americans is: \", acc_nsen)\n",
    "print(\"The total accuracy is: \", total_accuracy)\n",
    "\n",
    "# get calibration for the validation dataset\n",
    "\n",
    "calibration = calc_calibration(acc_sen, acc_nsen)\n",
    "\n",
    "print(\"The calibration is: \", calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47939212",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc2830",
   "metadata": {},
   "source": [
    "Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b81adb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Caucasians is:  0.6603325415676959\n",
      "The accuracy for African-Americans is:  0.5480314960629922\n",
      "The total accuracy is:  0.5928030303030303\n",
      "The calibration is:  0.11230104550470377\n"
     ]
    }
   ],
   "source": [
    "# get predictions for the testing dataset\n",
    "\n",
    "pred_test_s, pred_test_n = predict(loaded_model, X_test_s, X_test_n, 10)\n",
    "\n",
    "# get accuracy for the testing dataset\n",
    "\n",
    "acc_sen, acc_nsen, total_accuracy = calc_accuracy(pred_test_s, pred_test_n, y_test_s, y_test_n)\n",
    "\n",
    "print(\"The accuracy for Caucasians is: \", acc_sen)\n",
    "print(\"The accuracy for African-Americans is: \", acc_nsen)\n",
    "print(\"The total accuracy is: \", total_accuracy)\n",
    "\n",
    "# get calibration for the testing dataset\n",
    "\n",
    "calibration = calc_calibration(acc_sen, acc_nsen)\n",
    "\n",
    "print(\"The calibration is: \", calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aa1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
