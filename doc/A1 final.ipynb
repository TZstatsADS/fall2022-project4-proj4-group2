{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.optimize as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean, std\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Edit & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7214, 53)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data=pd.read_csv('/Users/zhangyudan/Documents/Github/fall2022-project4-proj4-group2-main/data/compas-scores-two-years.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we are only intersted in sample fairness between two races: African-American and Caucasian\n",
    "#and LFR needs to cageorize sensitive group and nonsensitive group,we regard defandant with African-American as 0,Caucasian as 1\n",
    "data = data[(data['race']=='African-American') | (data['race']=='Caucasian')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                            0\n",
      "end                           0\n",
      "start                         0\n",
      "priors_count.1                0\n",
      "v_screening_date              0\n",
      "v_score_text                  0\n",
      "v_decile_score                0\n",
      "v_type_of_assessment          0\n",
      "screening_date                0\n",
      "score_text                    0\n",
      "decile_score.1                0\n",
      "type_of_assessment            0\n",
      "is_violent_recid              0\n",
      "event                         0\n",
      "is_recid                      0\n",
      "c_charge_degree               0\n",
      "two_year_recid                0\n",
      "juv_fel_count                 0\n",
      "sex                           0\n",
      "compas_screening_date         0\n",
      "age                           0\n",
      "decile_score                  0\n",
      "last                          0\n",
      "juv_other_count               0\n",
      "dob                           0\n",
      "first                         0\n",
      "age_cat                       0\n",
      "name                          0\n",
      "race                          0\n",
      "priors_count                  0\n",
      "juv_misd_count                0\n",
      "c_days_from_compas           14\n",
      "c_case_number                14\n",
      "c_charge_desc                21\n",
      "out_custody                 180\n",
      "in_custody                  180\n",
      "days_b_screening_arrest     235\n",
      "c_jail_in                   235\n",
      "c_jail_out                  235\n",
      "c_offense_date              999\n",
      "r_charge_degree            3089\n",
      "r_offense_date             3089\n",
      "r_case_number              3089\n",
      "r_charge_desc              3141\n",
      "r_jail_out                 4087\n",
      "r_jail_in                  4087\n",
      "r_days_from_arrest         4087\n",
      "c_arrest_date              5165\n",
      "vr_offense_date            5433\n",
      "vr_charge_degree           5433\n",
      "vr_charge_desc             5433\n",
      "vr_case_number             5433\n",
      "violent_recid              6150\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# find the number of nan for each column\n",
    "nan_counts = data.isna().sum()\n",
    "print(nan_counts.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that having too many Nan\n",
    "data = data.dropna(thresh = len(data) - 21, axis = 1)\n",
    "# drop unnecessary columns \n",
    "data = data.drop(columns = ['id', 'name', 'first', 'last','compas_screening_date','c_case_number','dob','age','screening_date','v_screening_date','type_of_assessment','v_type_of_assessment'])\n",
    "# drop rows with Nan \n",
    "data = data.dropna()\n",
    "# The recidivist flag (is_recid) should be -1 if we could not find a compas case at all.\n",
    "data = data.loc[data['is_recid'] != -1]\n",
    "# Ordinary traffic offenses (c_charge_degree = 'O') will not result in Jail time and hence are removed \n",
    "data = data.loc[data['c_charge_degree'] != 'O']\n",
    "# score_text shouldn't be 'N/A'\n",
    "data = data.loc[data['score_text'] != 'N/A']\n",
    "# convert Series to int\n",
    "data.c_charge_desc = pd.to_numeric(data.c_charge_desc, errors='coerce').fillna(0, downcast='infer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_charge_desc            1\n",
      "is_violent_recid         2\n",
      "is_recid                 2\n",
      "event                    2\n",
      "c_charge_degree          2\n",
      "sex                      2\n",
      "race                     2\n",
      "two_year_recid           2\n",
      "age_cat                  3\n",
      "score_text               3\n",
      "v_score_text             3\n",
      "juv_other_count          9\n",
      "decile_score            10\n",
      "juv_fel_count           10\n",
      "decile_score.1          10\n",
      "v_decile_score          10\n",
      "juv_misd_count          10\n",
      "priors_count            37\n",
      "priors_count.1          37\n",
      "start                  221\n",
      "c_days_from_compas     454\n",
      "end                   1091\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_count = data.nunique()\n",
    "print(unique_count.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before building the LFR model, we need to transfer variables to dummy variables .\n",
    "- Split data into Sensitive and Nonsensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing values\n",
    "data['sex'].replace(['Male','Female'],[1,0], inplace = True)\n",
    "data['race'].replace(['Caucasian','African-American'],[1,0], inplace = True)\n",
    "data['c_charge_degree'].replace(['M','F'],[1,0], inplace = True)\n",
    "\n",
    "data['age_cat'].replace(['Less than 25','25 - 45','Greater than 45'],['A','B','C'], inplace = True)\n",
    "data.loc[data['age_cat']=='A', 'age_cat1'] = 1\n",
    "data.loc[data['age_cat']!='A', 'age_cat1'] = 0\n",
    "data.loc[data['age_cat']=='B', 'age_cat2'] = 1\n",
    "data.loc[data['age_cat']!='B', 'age_cat2'] = 0\n",
    "\n",
    "data['v_score_text'].replace(['Low','Medium','High'],['C','B','A'], inplace = True)\n",
    "data.loc[data['v_score_text']=='A', 'v_score_text1'] = 1\n",
    "data.loc[data['v_score_text']!='A', 'v_score_text1'] = 0\n",
    "data.loc[data['v_score_text']=='B', 'v_score_text2'] = 1\n",
    "data.loc[data['v_score_text']!='B', 'v_score_text2'] = 0\n",
    "\n",
    "data['score_text'].replace(['Low','Medium','High'],['B','C','A'], inplace = True)\n",
    "data.loc[data['score_text']=='A', 'score_text1'] = 1\n",
    "data.loc[data['score_text']!='A', 'score_text1'] = 0\n",
    "data.loc[data['score_text']=='B', 'score_text2'] = 1\n",
    "data.loc[data['score_text']!='B', 'score_text2'] = 0\n",
    "\n",
    "# drop original categorical variables\n",
    "data=data.drop(columns=['age_cat','v_score_text','score_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6129, 25)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>c_charge_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>age_cat1</th>\n",
       "      <th>age_cat2</th>\n",
       "      <th>v_score_text1</th>\n",
       "      <th>v_score_text2</th>\n",
       "      <th>score_text1</th>\n",
       "      <th>score_text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7207</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>529</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7208</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6129 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  race  juv_fel_count  decile_score  juv_misd_count  juv_other_count  \\\n",
       "1       1     0              0             3               0                0   \n",
       "2       1     0              0             4               0                1   \n",
       "3       1     0              0             8               1                0   \n",
       "6       1     1              0             6               0                0   \n",
       "8       0     1              0             1               0                0   \n",
       "...   ...   ...            ...           ...             ...              ...   \n",
       "7207    1     0              0             2               0                0   \n",
       "7208    1     0              0             9               0                0   \n",
       "7209    1     0              0             7               0                0   \n",
       "7210    1     0              0             3               0                0   \n",
       "7212    0     0              0             2               0                0   \n",
       "\n",
       "      priors_count  c_days_from_compas  c_charge_degree  c_charge_desc  ...  \\\n",
       "1                0                 1.0                0              0  ...   \n",
       "2                4                 1.0                0              0  ...   \n",
       "3                1                 1.0                0              0  ...   \n",
       "6               14                 1.0                0              0  ...   \n",
       "8                0                 1.0                1              0  ...   \n",
       "...            ...                 ...              ...            ...  ...   \n",
       "7207             0                 1.0                1              0  ...   \n",
       "7208             0                 1.0                0              0  ...   \n",
       "7209             0                 1.0                0              0  ...   \n",
       "7210             0                 1.0                0              0  ...   \n",
       "7212             3                 1.0                1              0  ...   \n",
       "\n",
       "      start   end  event  two_year_recid  age_cat1  age_cat2  v_score_text1  \\\n",
       "1         9   159      1               1       0.0       1.0            0.0   \n",
       "2         0    63      0               1       1.0       0.0            0.0   \n",
       "3         0  1174      0               0       1.0       0.0            0.0   \n",
       "6         5    40      1               1       0.0       1.0            0.0   \n",
       "8         2   747      0               0       0.0       1.0            0.0   \n",
       "...     ...   ...    ...             ...       ...       ...            ...   \n",
       "7207      0   529      1               1       0.0       1.0            0.0   \n",
       "7208      0   169      0               0       1.0       0.0            1.0   \n",
       "7209      1   860      0               0       1.0       0.0            0.0   \n",
       "7210      1   790      0               0       1.0       0.0            0.0   \n",
       "7212      0   754      0               0       0.0       1.0            0.0   \n",
       "\n",
       "      v_score_text2  score_text1  score_text2  \n",
       "1               0.0          0.0          1.0  \n",
       "2               0.0          0.0          1.0  \n",
       "3               1.0          1.0          0.0  \n",
       "6               0.0          0.0          0.0  \n",
       "8               0.0          0.0          1.0  \n",
       "...             ...          ...          ...  \n",
       "7207            0.0          0.0          1.0  \n",
       "7208            0.0          1.0          0.0  \n",
       "7209            1.0          0.0          0.0  \n",
       "7210            1.0          0.0          1.0  \n",
       "7212            0.0          0.0          1.0  \n",
       "\n",
       "[6129 rows x 25 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation and testing set (0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "data_train, data_val= train_test_split(data_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.concat([data_train.sex,data_train.race,data_train.juv_fel_count,data_train.decile_score,data_train.two_year_recid],axis=1)\n",
    "data_train=data_train.iloc[:20]\n",
    "data_val=pd.concat([data_val.sex,data_val.race,data_val.juv_fel_count,data_val.decile_score,data_val.two_year_recid],axis=1)\n",
    "data_test=pd.concat([data_test.sex,data_test.race,data_test.juv_fel_count,data_test.decile_score,data_test.two_year_recid],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6827</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  race  juv_fel_count  decile_score  two_year_recid\n",
       "4178    1     0              0             7               0\n",
       "2       1     0              0             4               1\n",
       "126     1     1              0             6               1\n",
       "6827    0     1              0             6               0\n",
       "4662    1     1              0             1               1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Fair Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d(x_n, v_k, \\alpha) = \\sum^D_{d=1} \\alpha_d (x_{nd} - v_{kd})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance - d(x_n, v_k, alpha)\n",
    "def distance(X, v, alpha):\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = len(v)\n",
    "\n",
    "    \n",
    "    res = np.zeros((N, K))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for d in range(D):\n",
    "                res[n, k] += alpha[d]*(X.iloc[n][d] - v[k, d])**2\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M_{nk} = P(Z=k|x_n) \\space\\space \\forall n, k \\\\ \n",
    "= exp(-d(x_n, v_k))/\\sum_{k=1}^K exp(-d(x_n, v_k))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_nk = P(Z=k|x_n)is the probablity that x_n maps to v_k\n",
    "\n",
    "def M_nk(dist, k):\n",
    "    \n",
    "    N = dist.shape[0]\n",
    "    K = dist.shape[1]\n",
    "    M_nk = np.zeros((N, K))\n",
    "    expo_res = np.zeros((N, K))\n",
    "    \n",
    "    for n in range(N):\n",
    "        deno = 0\n",
    "        for k in range(K):\n",
    "            expo_res[n, k] = math.exp((-1)*dist[n, k])\n",
    "            deno += expo_res[n, k]\n",
    "        for k in range(K):\n",
    "            M_nk[n, k] = expo_res[n, k] / deno\n",
    "    return M_nk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M_k = \\frac{1}{|X_0|} \\sum_{n \\in X_0} M_{nk}$\n",
    "\n",
    "$X_0 = \\{X_0^+, X_0^-\\}$\n",
    "\n",
    "$X_0$ is training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  M_k\n",
    "def M_k(X, M_nk, k):\n",
    "    N = X.shape[0]\n",
    "    K = M_nk.shape[1]\n",
    "    M_k = np.zeros(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            M_k[k] += M_nk[n, k]\n",
    "        M_k[k] = M_k[k]/N\n",
    "    return M_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\hat{x}_n = \\sum^K_{k=1}M_{nk}v_k\n",
    "$\n",
    "\n",
    "$\n",
    "L_x = \\sum_{n=1}^N (x_n - \\hat{x}_n)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the reconstruction of x_n  and L_x\n",
    "def x_n_hat(X, M_nk, v):\n",
    "\n",
    "    N = M_nk.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = M_nk.shape[1]\n",
    "    x_n_hat = np.zeros((N, D))\n",
    "    L_x = 0\n",
    "    \n",
    "    \n",
    "    for n in range(N):\n",
    "        for d in range(D):\n",
    "            for k in range(K):\n",
    "                x_n_hat[n, d] += M_nk[n, k]*v[k, d]\n",
    "        # calculate L_x        \n",
    "        L_x += (X.iloc[n][d] - x_n_hat[n, d])**2\n",
    "    \n",
    "    return x_n_hat, L_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "L_y = \\sum_{n=1}^N -y_n log \\hat{y}_n - (1-y_n)log(1- \\hat{y}_n)\n",
    "$\n",
    "\n",
    "\n",
    "minimize $L = A_z L_Z + A_x L_x + A_y L_y$ \n",
    "\n",
    "$A_x, A_z, A_y$ are hyperparameters governing trade-off between the system desiderata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for y_n and L_y\n",
    "def y_n_hat(M_nk, w, y):\n",
    "    N = M_nk.shape[0]\n",
    "    K = M_nk.shape[1]\n",
    "    y_n_hat = np.zeros(N)\n",
    "    L_y = 0\n",
    "    \n",
    "\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            y_n_hat[n] += M_nk[n, k]*w[k]\n",
    "        # calculate L_y\n",
    "        L_y += (-1)*y.iloc[n]*np.log(y_n_hat[n]) - (1 - y.iloc[n])*np.log(1 - y_n_hat[n])\n",
    "        \n",
    "    return y_n_hat, L_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the metric function we wanna minimize\n",
    "def L(param, sen_df, nsen_df, sen_y, nonsen_y, K, A_z, A_x, A_y):\n",
    "    # param is the list of parameters\n",
    "    # sen_df is the sensitive dataset\n",
    "    # nsen_df is the nonsensitive dataset\n",
    "    # sen_y is the list of labels for sensitive dataset\n",
    "    # nonsen_y is the list of labels for nonsensitive dataset\n",
    "    # K, A_z, A_x and A_y are hyperparameters, the values are decided by the users\n",
    "    \n",
    "    sen_N, sen_D = sen_df.shape\n",
    "    nsen_N, nsen_D = nsen_df.shape\n",
    "\n",
    "   \n",
    "    alpha_sen = param[:sen_D]\n",
    "    alpha_nsen = param[sen_D : 2 * sen_D]\n",
    "    w = param[2 * sen_D : (2 * sen_D) + K]\n",
    "    v = np.matrix(param[(2 * sen_D) + K:]).reshape((K, sen_D))\n",
    "\n",
    "    #  the distance matrix\n",
    "    dist_sen = distance(sen_df, v, alpha_sen)\n",
    "    dist_nsen = distance(nsen_df, v, alpha_nsen)        \n",
    "\n",
    "    # M_nk matrix\n",
    "    M_nk_sen = M_nk(dist_sen, K)\n",
    "    M_nk_nsen = M_nk(dist_nsen, K)\n",
    "\n",
    "    #  M_k matrix\n",
    "    M_k_sen = M_k(sen_df, M_nk_sen, K)\n",
    "    M_k_nsen = M_k(nsen_df, M_nk_nsen, K)\n",
    "\n",
    "    #  L_z\n",
    "    L_z = 0\n",
    "    for k in range(K):\n",
    "        L_z += abs(M_k_sen[k] - M_k_nsen[k])\n",
    "\n",
    "    #  x_n_hat and L_x\n",
    "    x_n_hat_sen, L_x_sen = x_n_hat(sen_df, M_nk_sen, v)\n",
    "    x_n_hat_nsen, L_x_nsen = x_n_hat(nsen_df, M_nk_nsen, v)\n",
    "    L_x = L_x_sen + L_x_nsen\n",
    "\n",
    "    # y_n_hat and L_y\n",
    "    y_hat_sen, L_y_sen = y_n_hat(M_nk_sen, w, sen_y)\n",
    "    y_hat_nsen, L_y_nsen = y_n_hat(M_nk_nsen, w, nonsen_y)\n",
    "    L_y = L_y_sen + L_y_nsen\n",
    "\n",
    "    # the  metric function\n",
    "    metric = A_z*L_z + A_x*L_x + A_y*L_y\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\hat{y}_n = \\sum^K_{k=1} M_{nk}w_k \\\\\n",
    "0< w_k <1\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the threshold for y_n_hat to be 0 or 1\n",
    "def predic_threshold(preds):\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] >= 0.5:\n",
    "            preds[i] = 1\n",
    "        else:\n",
    "            preds[i] = 0\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculate y_n_hat by using the best parameters\n",
    "def cal_pred(params, D, K, sen_dt, nsen_dt, sen_label, nsen_label):\n",
    "    # form parameters in new forms\n",
    "    best_alpha_sen = params[:D]\n",
    "    best_alpha_nsen = params[D : 2 * D]\n",
    "    best_w = params[2 * D : (2 * D) + K]\n",
    "    best_v = np.matrix(params[(2 * D) + K:]).reshape((K, D))\n",
    "    \n",
    "    # calculate the distance matrix\n",
    "    best_dist_sen = distance(sen_dt, best_v, best_alpha_sen)\n",
    "    best_dist_nsen = distance(nsen_dt, best_v, best_alpha_nsen) \n",
    "    \n",
    "    # calculate the M_nk matrix\n",
    "    best_M_nk_sen = M_nk(best_dist_sen, K)\n",
    "    best_M_nk_nsen = M_nk(best_dist_nsen, K)\n",
    "    \n",
    "    # calculate the y_n_hat matrix\n",
    "    y_hat_sen, L_y_sen = y_n_hat(best_M_nk_sen, best_w, sen_label)\n",
    "    y_hat_nsen, L_y_nsen = y_n_hat(best_M_nk_nsen, best_w, nsen_label)\n",
    "    \n",
    "    return y_hat_sen, y_hat_nsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the  overall accuracy, \n",
    "# accuracy for  African-American group\n",
    "# accuracy for  Caucasian group\n",
    "# and calibration of the model\n",
    "def cal_calibr(y_pred_sen, y_pred_nsen, y_sen_label, y_nsen_label):\n",
    "    converted_y_hat_sen = predic_threshold(y_pred_sen)\n",
    "    converted_y_hat_nsen = predic_threshold(y_pred_nsen)\n",
    "\n",
    "    y_pred_sen = pd.DataFrame(converted_y_hat_sen)\n",
    "    y_pred_nsen = pd.DataFrame(converted_y_hat_nsen)\n",
    "     \n",
    "    # calculate the accuracy\n",
    "    acc_sen = accuracy_score(y_sen_label, y_pred_sen)\n",
    "    acc_nsen = accuracy_score(y_nsen_label, y_pred_nsen)\n",
    "    \n",
    "    all_labels = y_sen_label.append(y_nsen_label)\n",
    "    all_preds = y_pred_sen.append(y_pred_nsen)\n",
    "    total_accuracy = accuracy_score(all_preds, all_labels)\n",
    "    \n",
    "    print(\"The overall accuracy  is: \", total_accuracy)\n",
    "    print(\"The accuracy for African-American  is: \", acc_sen)\n",
    "    print(\"The accuracy for Caucasian  is: \", acc_nsen)\n",
    "    print(\"The calibration of the model is: \", abs(acc_sen-acc_nsen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LFR function \n",
    "#get train /validation accuracy\n",
    "def LFR(training_data, val_data, y_name, sen_variable_name, K, A_z, A_x, A_y):\n",
    "    # dividing the training dataset into sensitive and nonsensitive group\n",
    "    sen_training = training_data[training_data[sen_variable_name]==0]\n",
    "    nsen_training = training_data[training_data[sen_variable_name]==1]\n",
    "    \n",
    "    # dividing the validation dataset \n",
    "    sen_val = val_data[val_data[sen_variable_name]==0]\n",
    "    nsen_val = val_data[val_data[sen_variable_name]==1]\n",
    "    \n",
    "    # delete sensitive variable \n",
    "    sen_training=sen_training.drop(columns=[sen_variable_name])\n",
    "    sen_val=sen_val.drop(columns=[sen_variable_name])  \n",
    "    nsen_training = nsen_training.drop(columns=[sen_variable_name])\n",
    "    nsen_val = nsen_val.drop(columns=[sen_variable_name])\n",
    "    \n",
    "    # assign y labels \n",
    "    y_sen_training = sen_training[y_name]\n",
    "    sen_training = sen_training.drop(columns=[y_name])\n",
    "    y_sen_val = sen_val[y_name]\n",
    "    sen_val = sen_val.drop(columns=[y_name])\n",
    "    y_nsen_training = nsen_training[y_name]\n",
    "    nsen_training = nsen_training.drop(columns=[y_name])\n",
    "    y_nsen_val = nsen_val[y_name]\n",
    "    nsen_val = nsen_val.drop(columns=[y_name])\n",
    "    \n",
    "    # pick random value\n",
    "    # talpha and w  are between 0 and 1 and sum up to 1\n",
    "    alpha_sen_1=np.random.random_sample((sen_training.shape[1],))\n",
    "    alpha_nsen_1=np.random.random_sample((nsen_training.shape[1],))\n",
    "    alpha_sen=alpha_sen_1/sum(alpha_sen_1)\n",
    "    alpha_nsen=alpha_nsen_1/sum(alpha_nsen_1)\n",
    "    w_1=np.random.random_sample((K,))\n",
    "    w=w_1/sum(w_1)\n",
    "    v=np.random.random((K, sen_training.shape[1]))\n",
    "    \n",
    "    \n",
    "    initial = []\n",
    "    initial.extend(alpha_sen)\n",
    "    initial.extend(alpha_nsen)\n",
    "    initial.extend(w)\n",
    "    \n",
    "    for item in v:\n",
    "        initial.extend(item)\n",
    "    initial = np.array(initial)\n",
    "\n",
    "    # the boundary of the parameters\n",
    "    bound=[]\n",
    "\n",
    "    #  alpha and w are between 0 and 1 and sum up to 1\n",
    "    for d in range(sen_training.shape[1]):\n",
    "        bound.append((0, 1))\n",
    "\n",
    "    for d in range(nsen_training.shape[1]):\n",
    "        bound.append((0, 1))\n",
    "\n",
    "    for k in range(K):\n",
    "        bound.append((0, 1))\n",
    "\n",
    "    \n",
    "    for k in range(K):\n",
    "        for d in range(sen_training.shape[1]):\n",
    "            bound.append((None, None))\n",
    "    \n",
    "    # minimize the metric \n",
    "    para, min_L, d = optim.fmin_l_bfgs_b(L, x0=initial, epsilon=1e-5, \n",
    "                                         args=(sen_training, nsen_training, y_sen_training, \n",
    "                                               y_nsen_training, K, A_z, A_x, A_y), \n",
    "                                         bounds = bound, approx_grad=True, \n",
    "                                         maxfun=150000, maxiter=150000)\n",
    "    \n",
    "    # predict y_n_hat for the training dataset \n",
    "    y_hat_sen_tr, y_hat_nsen_tr = cal_pred(para, sen_training.shape[1], K, sen_training, \n",
    "             nsen_training, y_sen_training, y_nsen_training)\n",
    "\n",
    "    print(\" training accuracy:\")\n",
    "    cal_calibr(y_hat_sen_tr, y_hat_nsen_tr, y_sen_training, y_nsen_training)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # predict y_n_hat for the validation dataset \n",
    "    y_hat_sen_val, y_hat_nsen_val = cal_pred(para, sen_val.shape[1], K, sen_val, \n",
    "             nsen_val, y_sen_val, y_nsen_val)  \n",
    "    print(\"validation accuracy:\")\n",
    "    cal_calibr(y_hat_sen_val, y_hat_nsen_val, y_sen_val, y_nsen_val)    \n",
    "    \n",
    "    return  para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training accuracy:\n",
      "The overall accuracy  is:  0.8\n",
      "The accuracy for African-American  is:  0.8181818181818182\n",
      "The accuracy for Caucasian  is:  0.7777777777777778\n",
      "The calibration of the model is:  0.04040404040404044\n",
      "validation accuracy:\n",
      "The overall accuracy  is:  0.5652528548123981\n",
      "The accuracy for African-American  is:  0.6215083798882681\n",
      "The accuracy for Caucasian  is:  0.48627450980392156\n",
      "The calibration of the model is:  0.13523387008434656\n",
      "The training time: 702.9030220508575\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "para_test = LFR(data_train, data_val, 'two_year_recid', 'race', 10, 0.3, 0.3, 0.4)\n",
    "end = time.time() \n",
    "print( f\"The training time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime of testing LFR model: 2.754948854446411\n",
      "The overall accuracy  is:  0.5603588907014682\n",
      "The accuracy for African-American  is:  0.6031537450722734\n",
      "The accuracy for Caucasian  is:  0.49032258064516127\n",
      "The calibration of the model is:  0.11283116442711211\n"
     ]
    }
   ],
   "source": [
    "# Testing the LFR model\n",
    "sen_test = data_test[data_test['race']==0]\n",
    "nsen_test = data_test[data_test['race']==1]\n",
    "\n",
    "sen_test=sen_test.drop(columns=['race'])\n",
    "nsen_test = nsen_test.drop(columns=['race'])\n",
    "\n",
    "y_sen_test = sen_test['two_year_recid']\n",
    "sen_test = sen_test.drop(columns=['two_year_recid'])\n",
    "\n",
    "y_nsen_test = nsen_test['two_year_recid']\n",
    "nsen_test = nsen_test.drop(columns=['two_year_recid'])\n",
    "\n",
    "start = time.time()\n",
    "y_hat_sen_test, y_hat_nsen_test = cal_pred(para_test, sen_test.shape[1], 10, sen_test, \n",
    "             nsen_test, y_sen_test, y_nsen_test)\n",
    "end = time.time() \n",
    "print( f\"runtime of testing LFR model: {end-start}\")\n",
    "cal_calibr(y_hat_sen_test, y_hat_nsen_test, y_sen_test, y_nsen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
